{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh.n\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from facenet_pytorch.models.mtcnn import PNet, RNet, ONet  # Import P-Net, R-Net, O-Net\n",
    "from facenet_pytorch import MTCNN\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import cv2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.nn.functional import normalize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_images_folder = r'D:\\Images\\Facial_reco_images\\Facial_reco_images\\Game'  # match photos\n",
    "studio_images_folder = r'E:\\2024\\hires\\324\\Game\\363155\\2024-JUNE 13-SATURDAY-TIME 4-FIELD 4-324-GAME-363155-SCHEDULE-249.JPG'  # studio photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fine-tuned P-Net, R-Net, and O-Net for finetuning\n",
    "class FinetunedMTCNN(MTCNN):\n",
    "#(self, image_size=160, margin=5, **kwargs): # use this after some time to improve the final results\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FinetunedMTCNN, self).__init__(**kwargs)\n",
    "        # Create your custom, finetuned P-Net, R-Net, O-Net here\n",
    "        self.pnet = PNet()\n",
    "        self.rnet = RNet()\n",
    "        self.onet = ONet()\n",
    "    def forward(self, x):\n",
    "        # Overriding forward pass if additional finetuning is needed\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FaceNet model\n",
    "facenet = InceptionResnetV1(pretrained='vggface2').eval()  # Load the FaceNet model\n",
    "# Initialize mtcnn model\n",
    "finetuned_mtcnn = FinetunedMTCNN(keep_all=True, device='cuda:0' if torch.cuda.is_available() else 'cpu', min_face_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_euclidean_distance(src, dst):\n",
    "    return np.linalg.norm(src - dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_procedure(img, left_eye, right_eye):    \n",
    "    left_eye_x, left_eye_y = left_eye\n",
    "    right_eye_x, right_eye_y = right_eye\n",
    "    \n",
    "    # Find the direction to rotate the image based on the eye coordinates\n",
    "    if left_eye_y > right_eye_y:\n",
    "        point_3rd = (right_eye_x, left_eye_y)\n",
    "        direction = -1  # Clockwise\n",
    "    else:\n",
    "        point_3rd = (left_eye_x, right_eye_y)\n",
    "        direction = 1  # Counter-clockwise\n",
    "    \n",
    "    # Calculate the length of the triangle edges\n",
    "    a = find_euclidean_distance(np.array(left_eye), np.array(point_3rd))\n",
    "    b = find_euclidean_distance(np.array(right_eye), np.array(point_3rd))\n",
    "    c = find_euclidean_distance(np.array(left_eye), np.array(right_eye))\n",
    "    \n",
    "    # Apply cosine rule to find the angle\n",
    "    if b != 0 and c != 0:  # Avoid division by zero\n",
    "        cos_a = (b**2 + c**2 - a**2) / (2 * b * c)\n",
    "        angle = np.arccos(cos_a)  # Angle in radians\n",
    "        angle = np.degrees(angle)  # Convert to degrees\n",
    "        \n",
    "        # Adjust the angle based on the rotation direction\n",
    "        if direction == -1:\n",
    "            angle = 90 - angle\n",
    "        \n",
    "        # Rotate the image using PIL\n",
    "        #img = Image.fromarray(img)\n",
    "        img = img.rotate(direction * angle, resample=Image.BICUBIC)\n",
    "        img = np.array(img)  # Convert back to numpy array\n",
    "        # aligned_img_pil = Image.fromarray(img)  # Convert back to PIL Image for saving\n",
    "        # aligned_img_pil.show()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform face detection and store image paths with their cropped face regions\n",
    "def process_and_save_match_faces(image_folder, mtcnn_model, margin=0):\n",
    "    global match_photos_all\n",
    "    match_photos_all = []\n",
    "    match_photos_paths = []\n",
    "    image = Image.open(image_folder)\n",
    "    boxes, confidences, landmarks = mtcnn_model.detect(image, landmarks=True)\n",
    "    if boxes is not None:\n",
    "        # Set a confidence threshold\n",
    "        threshold = 0.95\n",
    "        # Filter detected faces based on the confidence score\n",
    "        filtered_faces = [i for i, confidence in enumerate(confidences) if confidence > threshold]\n",
    "        #print(f\"Number of filtered faces: {len(filtered_faces)}\")\n",
    "        # Process each filtered face\n",
    "        for i in filtered_faces:\n",
    "            box = boxes[i]  # Get the bounding box for the filtered face\n",
    "            box = [int(b) for b in box]  # Ensure the box is in integer format\n",
    "            # adding margin around the box\n",
    "            # Apply margin to the bounding box\n",
    "            x1 = max(0, box[0] - margin)  # Left\n",
    "            y1 = max(0, box[1] - margin)  # Top\n",
    "            x2 = min(image.width, box[2] + margin)  # Right\n",
    "            y2 = min(image.height, box[3] + margin)  # Bottom\n",
    "            # Crop the face from the image\n",
    "            cropped_face = image.crop((x1, y1, x2, y2))\n",
    "            if cropped_face is not None:\n",
    "                # Get the landmarks (left and right eyes) for the current face\n",
    "                left_eye = landmarks[i][0]  # Left eye coordinates for face i\n",
    "                right_eye = landmarks[i][1]  # Right eye coordinates for face i\n",
    "                # Align the cropped face using the eye coordinates\n",
    "                aligned_face = alignment_procedure(cropped_face, left_eye, right_eye)\n",
    "                # Store the aligned face and the original image path\n",
    "                match_photos_all.append(aligned_face)\n",
    "                match_photos_paths.append(image_folder)  # Store original image path\n",
    "    return match_photos_paths  # Return list of original image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_studio_faces(image_folder, mtcnn_model, margin=0):\n",
    "    global studio_photos_all\n",
    "    studio_photos_all = []\n",
    "    studio_photos_paths = []\n",
    "    image = Image.open(image_folder)\n",
    "    # Detect faces (with bounding boxes, probabilities, and landmarks)\n",
    "    boxes, confidences, landmarks = mtcnn_model.detect(image, landmarks=True)\n",
    "    if boxes is not None:\n",
    "        # Set a confidence threshold\n",
    "        threshold = 0.95\n",
    "        # Filter detected faces based on the confidence score\n",
    "        filtered_faces = [i for i, confidence in enumerate(confidences) if confidence > threshold]\n",
    "        # Save original image paths and face regions that pass the confidence threshold\n",
    "        for i in filtered_faces:\n",
    "            box = boxes[i]  # Get the bounding box for the filtered face\n",
    "            box = [int(b) for b in box]  # Ensure the box is in integer format\n",
    "            # Apply margin to the bounding box\n",
    "            x1 = max(0, box[0] - margin)  # Left\n",
    "            y1 = max(0, box[1] - margin)  # Top\n",
    "            x2 = min(image.width, box[2] + margin)  # Right\n",
    "            y2 = min(image.height, box[3] + margin)  # Bottom\n",
    "            # Crop the face from the image\n",
    "            cropped_face = image.crop((x1, y1, x2, y2))\n",
    "            if cropped_face is not None:\n",
    "                left_eye = landmarks[i][0]\n",
    "                #print(left_eye)\n",
    "                right_eye = landmarks[i][1]\n",
    "                #print(right_eye)\n",
    "                aligned_face = alignment_procedure(cropped_face, left_eye, right_eye)\n",
    "                studio_photos_all.append(aligned_face)\n",
    "                studio_photos_paths.append(image_folder)  # Store original image path\n",
    "    return studio_photos_paths  # Return list of original image paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_embedding(image):\n",
    "    image = cv2.resize(image, (160, 160))\n",
    "    image = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "    embedding = facenet(image)\n",
    "    embedding = normalize(embedding, p=2, dim=1)  # L2 normalization of embeddings\n",
    "    return embedding.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Cosine Similarity between embeddings\n",
    "def cosine_similarity_metric(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of images in the list studio_photos_all: [(634, 574, 3)]\n",
      "Number of studio photos: 1\n"
     ]
    }
   ],
   "source": [
    "# Process both match and studio images, and return original image paths\n",
    "studio_photos_paths = process_and_save_studio_faces(studio_images_folder, finetuned_mtcnn, margin=200)\n",
    "studio_image_shapes = [np.array(image).shape for image in studio_photos_all]\n",
    "print(\"Shapes of images in the list studio_photos_all:\", studio_image_shapes)\n",
    "print(f\"Number of studio photos: {len(studio_photos_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for studio photos\n",
    "studio_embeddings = []\n",
    "for i, studio_photo in enumerate(studio_photos_all):\n",
    "    studio_image = np.array(studio_photo)  # Convert the PIL image to a NumPy array\n",
    "    embedding = get_face_embedding(studio_image)  # Get embedding\n",
    "    if embedding is not None:\n",
    "        studio_embeddings.append((i, embedding))  # Store the index and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(studio_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of images in the list match_photos_all: [(314, 254, 3)]\n",
      "Number of match photos: 1\n"
     ]
    }
   ],
   "source": [
    "# Process and return original match photo paths\n",
    "match_photos_paths = process_and_save_match_faces(match_images_folder, finetuned_mtcnn, margin=40)\n",
    "match_image_shapes = [np.array(image).shape for image in match_photos_all]\n",
    "print(\"Shapes of images in the list match_photos_all:\", match_image_shapes)\n",
    "print(f\"Number of match photos: {len(match_photos_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_face_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, match_photo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(match_photos_all):\n\u001b[0;32m      4\u001b[0m     match_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(match_photo)  \u001b[38;5;66;03m# Convert the PIL image to a NumPy array\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_face_embedding\u001b[49m(match_image)  \u001b[38;5;66;03m# Get embedding\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m         game_embeddings\u001b[38;5;241m.\u001b[39mappend((i, embedding))  \u001b[38;5;66;03m# Store the index and embedding\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_face_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for match (game) photos\n",
    "game_embeddings = []\n",
    "for i, match_photo in enumerate(match_photos_all):\n",
    "    match_image = np.array(match_photo)  # Convert the PIL image to a NumPy array\n",
    "    embedding = get_face_embedding(match_image)  # Get embedding\n",
    "    if embedding is not None:\n",
    "        game_embeddings.append((i, embedding))  # Store the index and embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare studio embeddings with match embeddings\n",
    "results = {}\n",
    "threshold = 0.4  # Define a similarity threshold for matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m studio_idx, studio_embedding \u001b[38;5;129;01min\u001b[39;00m studio_embeddings:\n\u001b[0;32m      2\u001b[0m     matched_photos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m game_idx, game_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgame_embeddings\u001b[49m:\n\u001b[0;32m      4\u001b[0m         similarity \u001b[38;5;241m=\u001b[39m cosine_similarity_metric(studio_embedding, game_embedding)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m threshold:  \u001b[38;5;66;03m# If similarity is above the threshold, consider it a match\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'game_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "for studio_idx, studio_embedding in studio_embeddings:\n",
    "    matched_photos = []\n",
    "    for game_idx, game_embedding in game_embeddings:\n",
    "        similarity = cosine_similarity_metric(studio_embedding, game_embedding)\n",
    "        if similarity > threshold:  # If similarity is above the threshold, consider it a match\n",
    "            matched_photos.append((game_idx, similarity))\n",
    "    results[studio_idx] = matched_photos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results, but load and display the original images (using the file paths)\n",
    "for studio_idx, matches in results.items():\n",
    "    print(f'Matching game photos for studio photo index {studio_idx}:')\n",
    "    print(matches)  # List of matched game photos (indices and distances)\n",
    "    if matches:\n",
    "        # Load and display the studio image using the file path\n",
    "        studio_image = Image.open(studio_photos_paths[studio_idx])\n",
    "        # Set up the plot for studio and matched game images\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        plt.subplot(1, len(matches) + 1, 1)\n",
    "        plt.imshow(studio_image)\n",
    "        plt.title(f'Studio Photo Index: {studio_idx}')\n",
    "        plt.axis('off')\n",
    "        # Display each matched game image using the original file path\n",
    "        for i, (game_idx, distance) in enumerate(matches):\n",
    "            game_image = Image.open(match_photos_paths[game_idx])  # Load the match image by path\n",
    "            plt.subplot(1, len(matches) + 1, i + 2)\n",
    "            plt.imshow(game_image)\n",
    "            plt.title(f'Game Photo Index: {game_idx}\\nDistance: {distance:.4f}')\n",
    "            plt.axis('off')\n",
    "        # Show the plot with studio and match images\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No matches found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
